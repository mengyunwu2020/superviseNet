% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/superviseNet.R
\name{superviseNet}
\alias{superviseNet}
\title{Supervised heterogeneous network estimation via survival-based Bayesian graphical models  (superviseNet).}
\usage{
superviseNet(
  ct,
  xx,
  K,
  lambda_mu,
  lambda_b,
  v_0,
  v_1 = 1,
  p_2,
  lambda_s,
  l.m,
  member_input,
  eps = 0.01,
  maxiter = 50,
  threshold = 0.001,
  eps_z = 1e-05,
  l.update = TRUE,
  tau1 = 0.001
)
}
\arguments{
\item{ct}{A two-column matrix with the first column being the survival time and the second column being the censoring indicator. The indicator is a binary variable, with "1" indicating dead, and "0" indicating right censored.}

\item{xx}{Input matrix of p genetic measurements consisting of n rows. Each row is an observation vector.}

\item{K}{Number of subgroups.}

\item{lambda_mu}{A non-negative tuning parameter of laplace prior on subgroup mean parameters.}

\item{lambda_b}{A non-negative tuning parameter of laplace prior on subgroup regression parameters.}

\item{v_0}{The spike prior parameter.}

\item{v_1}{The slab prior parameter.}

\item{p_2}{The probability parameter of Bernoulli prior on binary latent indicator $gamma_{k,jl}$.}

\item{lambda_s}{A non-negative tuning parameter controlling the similarity across different networks.}

\item{l.m}{A vector composed of similarity-based matrix.}

\item{member_input}{A vector indicating initialized subgroup memberships of each subjects.}

\item{eps}{Tolerance for the EM algorithm.}

\item{maxiter}{Maximum of number of iterations.}

\item{threshold}{A small constant that thresholds the final precision matrix estimator.}

\item{eps_z}{A small regularized constant used in the risk ratio computation.}

\item{l.update}{Whether update the similarity matrix.}

\item{tau1}{A small constant used in the computation of similarity matrix.}
}
\value{
A list
}
\description{
Supervised heterogeneous network estimation via survival-based Bayesian graphical models  (superviseNet).
}
\examples{
n <- 150
p <- 100
K <- 3
set.seed(1)
mu01 = mu02 = mu03 = rep(0,p)
beta=matrix(0,K,p)
beta[1,1:5]=2
beta[2,1:5]=-2
beta[3,3:7]=1
beta0=c(0,0,0)
sigma=c(.01,.01,0.01)
rate=.8
Mu0.list <- list(mu01,mu02,mu03)
whole.data <- generate.data(n,Mu0.list,beta,beta0,sigma)
whole.data$beta0=beta
xx=whole.data$data
ct=whole.data$ct
set.seed(1)
class_old<-sample(1:K,nrow(xx),replace=TRUE)
Thetab <- array(0, dim = c(p, p, K))
for(k in 1:K)
{
Thetab[,,k] <- diag(1,p,p)
}
tau1=0.001
l.m=NULL
for(i in 1:(p-1)){
for(j in (i+1):p){
tmm=1/sqrt(Thetab[i,j,]^2+tau1^2)
tmp.m=-matrix(tmm,ncol=1)\%*\%matrix(tmm,nrow=1)
diag(tmp.m)=(K-1)*(tmm^2)
tmp=tmp.m
dddddd=det(tmp)
if(dddddd<0) print('det is not')
l.m=c(l.m,c(tmp))
}
}
res= superviseNet(ct,xx,K,lambda_mu=sqrt(dim(ct)[1]*log(p))/2,lambda_b=sqrt(dim(ct)[1]*log(p))/2,v_0=0.057,v_1=1,p_2=0.85,lambda_s=.01,l.m=l.m,member_input=class_old,eps=1e-2,maxiter=50,threshold=1e-3,eps_z=1e-5,l.update=TRUE,tau1=tau1)
}
